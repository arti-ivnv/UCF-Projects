{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LwvvMtG18SK"
      },
      "source": [
        "# Recurrent Neural Network Homework\n",
        "\n",
        "This is the 4th assignment for CAP 4630 and we will implement a basic RNN network and an LSTM network with Keras to solve two problems. \\\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total points 85, plus 15 bonus points)** \\\n",
        "You may use Machine Learning libaries like Scikit-learn for data preprocessing.\n",
        "\n",
        "**Task Overview:**\n",
        "- Implement a basic RNN network to solve time series prediction \n",
        "- Implement an LSTM network to conduct sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24oSrIK18SL"
      },
      "source": [
        "## 1 - Implement Basic RNN network with Keras to predict time series##\n",
        "### 1.1 Prepare the data (17 Points)\n",
        "\n",
        "Prepare time series data for deep neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the given train and test data: \"train.txt\" and \"test.txt\". **(5 Points)**\n",
        "2. Generate the **TRAIN** and **TEST** labels. **(5 Points)**\n",
        "2. Normalize the **TRAIN** and **TEST** data with sklearn function \"MinMaxScaler\". **(5 Points)**\n",
        "3. Print out the **TEST** data and label. **(2 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. The length of original train data is 113 which starts from **\"1949-01\"** to **\"1958-05\"**. The length of original test data is 29, which starts from **\"1958-07\"** to **\"1960-11\"**. \n",
        "2. Set the data types of both train and test data to \"float32\". \n",
        "3. When you prepared input data X (sequences) and oupt data Y (labels), please consider the following relationship:\n",
        "    - The sequence X should be the **past 12** datapoints in the time series, i.e., observation sequence with historical window of 12. You may check the time series data and think about the reason.\n",
        "    - The label Y should be the **next 1** datapoint in the time series (one point ahead prediction).\n",
        "4. The first 3 train data and label should be:\n",
        "\n",
        "trainX[0] = [[0.5801282 &nbsp; 0.625 &nbsp; 0.30128205 &nbsp;0.15705132 &nbsp;0. &nbsp; 0.08653843 &nbsp; 0.16025639 &nbsp; 0.1025641 &nbsp; 0.3076923 &nbsp; 0.27564108 &nbsp;0.3525641 &nbsp; 0.5192307]]\\\n",
        "trainY[0] = [0.7628205]\n",
        "\n",
        "trainX[1] = [[0.625   &nbsp;   0.30128205&nbsp; 0.15705132&nbsp; 0.   &nbsp;      0.08653843 &nbsp;0.16025639&nbsp; 0.1025641 &nbsp; 0. &nbsp;3076923&nbsp; 0.27564108 &nbsp;0.3525641&nbsp;  0.5192307&nbsp;  0.7628205 ]]\\\n",
        "trainY[1] = [0.798077]\n",
        "\n",
        "trainX[2] =  [[0.30128205 &nbsp; 0.15705132 &nbsp; 0.   &nbsp;   0.&nbsp; 08653843&nbsp;0.16025639&nbsp;0.1025641 &nbsp; 0.3076923 &nbsp;0.27564108 &nbsp;0.3525641 &nbsp; 0.5192307 &nbsp;0.7628205 &nbsp; 0.798077]]\\\n",
        "trainY[2] = [0.49038458]\n",
        "\n",
        "5. Apply the MinMaxScaler to both the train and test data.\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "\n",
        "\n",
        "6. After the preparation with scaler fitting, the shapes of trainX, trainY, testX, and testY are as follows:\\\n",
        "trainX.shape = (101, 1, 12)\\\n",
        "trainY.shape = (101,)\\\n",
        "testX.shape = (17, 1, 12)\\\n",
        "testY.shape = (17,)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS7xhrC3-_-1"
      },
      "source": [
        "### Import Libraries ###\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "reset_random_seeds()"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shAj2Y6IuxUv"
      },
      "source": [
        "### Prepare and Preprocess Data Here ###\n",
        "\n",
        "from pandas import read_csv\n",
        "\n",
        "### Design a Function to Prepare Observation Sequences and Corresponding Labels ###\n",
        "\n",
        "def create_dataset(dataset, look_back=12): # look_back is used to specify input sequence length\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back): # 113 - 12 = 101 from 0 to 101\n",
        "        dataX.append(dataset[i:i+12]) # make sure correct start and end elements\n",
        "        dataY.append(dataset[i+12]) # make sure correct start and end elements; here, we have only one point ahead for prediction\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "\n",
        "### Train and Test Data Loading with float32 type ####\n",
        "\n",
        "dataframe_test = read_csv('test.txt',usecols=['Passengers'])\n",
        "dataset_test = dataframe_test.values\n",
        "dataset_test = dataset_test.astype('float32')\n",
        "\n",
        "dataframe_train = read_csv('train.txt',usecols=['Passengers'])\n",
        "dataset_train = dataframe_train.values\n",
        "dataset_train = dataset_train.astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8MlgYTvvIeD"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "### Scale Training and Test Data to [0, 1] ###\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) # specify the scaler\n",
        "train = scaler.fit_transform(dataset_train) # fit the scaler to the training data\n",
        "test = scaler.transform(dataset_test) # fit the scaler to the test data\n",
        "\n"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4kdVFcR23rU"
      },
      "source": [
        "### Train and Test Data Split\n",
        "#print(train)\n",
        "trainX, trainY = create_dataset(train, look_back=12) # historical window is 12; future window is 1.\n",
        "testX, testY = create_dataset(test, look_back=12)\n",
        "\n",
        "### Train and Test Data Reshape (to fit RNN input)\n",
        "\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1])) \n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ1uHWzX8wlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20cf5d8-2c20-4e3a-dde1-a1e28b9c1d4a"
      },
      "source": [
        "# print out the test data and label here\n",
        "\n",
        "print(testX)\n",
        "print(testY)\n"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1.0661157  1.1046832  0.8264463  0.70247936 0.5674931  0.64187324\n",
            "   0.70523417 0.6556474  0.8319559  0.8044077  0.87052345 1.013774  ]]\n",
            "\n",
            " [[1.1046832  0.8264463  0.70247936 0.5674931  0.64187324 0.70523417\n",
            "   0.6556474  0.8319559  0.8044077  0.87052345 1.013774   1.2231405 ]]\n",
            "\n",
            " [[0.8264463  0.70247936 0.5674931  0.64187324 0.70523417 0.6556474\n",
            "   0.8319559  0.8044077  0.87052345 1.013774   1.2231405  1.2534435 ]]\n",
            "\n",
            " [[0.70247936 0.5674931  0.64187324 0.70523417 0.6556474  0.8319559\n",
            "   0.8044077  0.87052345 1.013774   1.2231405  1.2534435  0.98898065]]\n",
            "\n",
            " [[0.5674931  0.64187324 0.70523417 0.6556474  0.8319559  0.8044077\n",
            "   0.87052345 1.013774   1.2231405  1.2534435  0.98898065 0.8347107 ]]\n",
            "\n",
            " [[0.64187324 0.70523417 0.6556474  0.8319559  0.8044077  0.87052345\n",
            "   1.013774   1.2231405  1.2534435  0.98898065 0.8347107  0.7107438 ]]\n",
            "\n",
            " [[0.70523417 0.6556474  0.8319559  0.8044077  0.87052345 1.013774\n",
            "   1.2231405  1.2534435  0.98898065 0.8347107  0.7107438  0.8292011 ]]\n",
            "\n",
            " [[0.6556474  0.8319559  0.8044077  0.87052345 1.013774   1.2231405\n",
            "   1.2534435  0.98898065 0.8347107  0.7107438  0.8292011  0.8622589 ]]\n",
            "\n",
            " [[0.8319559  0.8044077  0.87052345 1.013774   1.2231405  1.2534435\n",
            "   0.98898065 0.8347107  0.7107438  0.8292011  0.8622589  0.79063356]]\n",
            "\n",
            " [[0.8044077  0.87052345 1.013774   1.2231405  1.2534435  0.98898065\n",
            "   0.8347107  0.7107438  0.8292011  0.8622589  0.79063356 0.8677685 ]]\n",
            "\n",
            " [[0.87052345 1.013774   1.2231405  1.2534435  0.98898065 0.8347107\n",
            "   0.7107438  0.8292011  0.8622589  0.79063356 0.8677685  0.98347104]]\n",
            "\n",
            " [[1.013774   1.2231405  1.2534435  0.98898065 0.8347107  0.7107438\n",
            "   0.8292011  0.8622589  0.79063356 0.8677685  0.98347104 1.013774  ]]\n",
            "\n",
            " [[1.2231405  1.2534435  0.98898065 0.8347107  0.7107438  0.8292011\n",
            "   0.8622589  0.79063356 0.8677685  0.98347104 1.013774   1.1873279 ]]\n",
            "\n",
            " [[1.2534435  0.98898065 0.8347107  0.7107438  0.8292011  0.8622589\n",
            "   0.79063356 0.8677685  0.98347104 1.013774   1.1873279  1.4269972 ]]\n",
            "\n",
            " [[0.98898065 0.8347107  0.7107438  0.8292011  0.8622589  0.79063356\n",
            "   0.8677685  0.98347104 1.013774   1.1873279  1.4269972  1.3829201 ]]\n",
            "\n",
            " [[0.8347107  0.7107438  0.8292011  0.8622589  0.79063356 0.8677685\n",
            "   0.98347104 1.013774   1.1873279  1.4269972  1.3829201  1.1129477 ]]\n",
            "\n",
            " [[0.7107438  0.8292011  0.8622589  0.79063356 0.8677685  0.98347104\n",
            "   1.013774   1.1873279  1.4269972  1.3829201  1.1129477  0.98347104]]]\n",
            "[[1.2231405 ]\n",
            " [1.2534435 ]\n",
            " [0.98898065]\n",
            " [0.8347107 ]\n",
            " [0.7107438 ]\n",
            " [0.8292011 ]\n",
            " [0.8622589 ]\n",
            " [0.79063356]\n",
            " [0.8677685 ]\n",
            " [0.98347104]\n",
            " [1.013774  ]\n",
            " [1.1873279 ]\n",
            " [1.4269972 ]\n",
            " [1.3829201 ]\n",
            " [1.1129477 ]\n",
            " [0.98347104]\n",
            " [0.78787875]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlakqA_vuFb"
      },
      "source": [
        "### 1.2 - Build the RNN model (20 Points) ##\n",
        "\n",
        "\n",
        "Build an RNN model with SimpleRNN cell. \n",
        "\n",
        "**Tasks:**\n",
        "1. Build an RNN model with 1 RNN layer and 1 Dense layer.  **(10 Points)**\n",
        "2. Compile the model. **(5 Points)**\n",
        "3. Train the model for 100 epochs with **batch_size = 10**. **(5 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may consider **tensorflow.keras.layers.SimpleRNN(unit_size=4)** to specify RNN cells.\n",
        "2. Use loss function = 'mean_squared_error' and select **Adam** optimizer with **learning_rate=0.01** and other default settings.\n",
        "3. After first epoch, the train loss is changed to around **0.0656**. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn92qh8oyq0B"
      },
      "source": [
        "### Build the RNN Model ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = Sequential() # Declare Sequential class and assign it to variable \"model\"\n",
        "model.add(keras.layers.SimpleRNN(units=4)) # Add a simple RNN layer with unit_size=4 in the model \n",
        "model.add(keras.layers.Dense(1)) # Add a following Dense layer with units=1 in the model \n"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnO-5WT-3hgH"
      },
      "source": [
        "### Compile the RNN model ###\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='mean_squared_error',optimizer='adam') # model compiled with mean_squared_error loss and adam optimizer"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tpZAutlzify",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d685a04-93c9-4a81-ea20-e72b4296d52c"
      },
      "source": [
        "### Train the RNN model and PRINT OUT MODEL STRUCTURE with model.summary() ###\n",
        "\n",
        "model.fit(trainX,trainY,epochs=100,batch_size=10, verbose=2) # model fit with epoch=100, batch_size=10; verbose=2 is optional.\n",
        "model.summary() # print out model structure with model.summary()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 - 0s - loss: 0.1754\n",
            "Epoch 2/100\n",
            "11/11 - 0s - loss: 0.1121\n",
            "Epoch 3/100\n",
            "11/11 - 0s - loss: 0.0695\n",
            "Epoch 4/100\n",
            "11/11 - 0s - loss: 0.0433\n",
            "Epoch 5/100\n",
            "11/11 - 0s - loss: 0.0317\n",
            "Epoch 6/100\n",
            "11/11 - 0s - loss: 0.0268\n",
            "Epoch 7/100\n",
            "11/11 - 0s - loss: 0.0246\n",
            "Epoch 8/100\n",
            "11/11 - 0s - loss: 0.0240\n",
            "Epoch 9/100\n",
            "11/11 - 0s - loss: 0.0235\n",
            "Epoch 10/100\n",
            "11/11 - 0s - loss: 0.0230\n",
            "Epoch 11/100\n",
            "11/11 - 0s - loss: 0.0226\n",
            "Epoch 12/100\n",
            "11/11 - 0s - loss: 0.0218\n",
            "Epoch 13/100\n",
            "11/11 - 0s - loss: 0.0216\n",
            "Epoch 14/100\n",
            "11/11 - 0s - loss: 0.0212\n",
            "Epoch 15/100\n",
            "11/11 - 0s - loss: 0.0206\n",
            "Epoch 16/100\n",
            "11/11 - 0s - loss: 0.0201\n",
            "Epoch 17/100\n",
            "11/11 - 0s - loss: 0.0197\n",
            "Epoch 18/100\n",
            "11/11 - 0s - loss: 0.0194\n",
            "Epoch 19/100\n",
            "11/11 - 0s - loss: 0.0191\n",
            "Epoch 20/100\n",
            "11/11 - 0s - loss: 0.0187\n",
            "Epoch 21/100\n",
            "11/11 - 0s - loss: 0.0183\n",
            "Epoch 22/100\n",
            "11/11 - 0s - loss: 0.0178\n",
            "Epoch 23/100\n",
            "11/11 - 0s - loss: 0.0174\n",
            "Epoch 24/100\n",
            "11/11 - 0s - loss: 0.0171\n",
            "Epoch 25/100\n",
            "11/11 - 0s - loss: 0.0167\n",
            "Epoch 26/100\n",
            "11/11 - 0s - loss: 0.0163\n",
            "Epoch 27/100\n",
            "11/11 - 0s - loss: 0.0159\n",
            "Epoch 28/100\n",
            "11/11 - 0s - loss: 0.0157\n",
            "Epoch 29/100\n",
            "11/11 - 0s - loss: 0.0153\n",
            "Epoch 30/100\n",
            "11/11 - 0s - loss: 0.0148\n",
            "Epoch 31/100\n",
            "11/11 - 0s - loss: 0.0146\n",
            "Epoch 32/100\n",
            "11/11 - 0s - loss: 0.0143\n",
            "Epoch 33/100\n",
            "11/11 - 0s - loss: 0.0139\n",
            "Epoch 34/100\n",
            "11/11 - 0s - loss: 0.0137\n",
            "Epoch 35/100\n",
            "11/11 - 0s - loss: 0.0133\n",
            "Epoch 36/100\n",
            "11/11 - 0s - loss: 0.0130\n",
            "Epoch 37/100\n",
            "11/11 - 0s - loss: 0.0128\n",
            "Epoch 38/100\n",
            "11/11 - 0s - loss: 0.0125\n",
            "Epoch 39/100\n",
            "11/11 - 0s - loss: 0.0125\n",
            "Epoch 40/100\n",
            "11/11 - 0s - loss: 0.0121\n",
            "Epoch 41/100\n",
            "11/11 - 0s - loss: 0.0116\n",
            "Epoch 42/100\n",
            "11/11 - 0s - loss: 0.0114\n",
            "Epoch 43/100\n",
            "11/11 - 0s - loss: 0.0111\n",
            "Epoch 44/100\n",
            "11/11 - 0s - loss: 0.0109\n",
            "Epoch 45/100\n",
            "11/11 - 0s - loss: 0.0108\n",
            "Epoch 46/100\n",
            "11/11 - 0s - loss: 0.0106\n",
            "Epoch 47/100\n",
            "11/11 - 0s - loss: 0.0102\n",
            "Epoch 48/100\n",
            "11/11 - 0s - loss: 0.0100\n",
            "Epoch 49/100\n",
            "11/11 - 0s - loss: 0.0098\n",
            "Epoch 50/100\n",
            "11/11 - 0s - loss: 0.0095\n",
            "Epoch 51/100\n",
            "11/11 - 0s - loss: 0.0095\n",
            "Epoch 52/100\n",
            "11/11 - 0s - loss: 0.0091\n",
            "Epoch 53/100\n",
            "11/11 - 0s - loss: 0.0089\n",
            "Epoch 54/100\n",
            "11/11 - 0s - loss: 0.0087\n",
            "Epoch 55/100\n",
            "11/11 - 0s - loss: 0.0086\n",
            "Epoch 56/100\n",
            "11/11 - 0s - loss: 0.0083\n",
            "Epoch 57/100\n",
            "11/11 - 0s - loss: 0.0081\n",
            "Epoch 58/100\n",
            "11/11 - 0s - loss: 0.0079\n",
            "Epoch 59/100\n",
            "11/11 - 0s - loss: 0.0078\n",
            "Epoch 60/100\n",
            "11/11 - 0s - loss: 0.0076\n",
            "Epoch 61/100\n",
            "11/11 - 0s - loss: 0.0075\n",
            "Epoch 62/100\n",
            "11/11 - 0s - loss: 0.0073\n",
            "Epoch 63/100\n",
            "11/11 - 0s - loss: 0.0071\n",
            "Epoch 64/100\n",
            "11/11 - 0s - loss: 0.0068\n",
            "Epoch 65/100\n",
            "11/11 - 0s - loss: 0.0068\n",
            "Epoch 66/100\n",
            "11/11 - 0s - loss: 0.0066\n",
            "Epoch 67/100\n",
            "11/11 - 0s - loss: 0.0065\n",
            "Epoch 68/100\n",
            "11/11 - 0s - loss: 0.0063\n",
            "Epoch 69/100\n",
            "11/11 - 0s - loss: 0.0061\n",
            "Epoch 70/100\n",
            "11/11 - 0s - loss: 0.0060\n",
            "Epoch 71/100\n",
            "11/11 - 0s - loss: 0.0058\n",
            "Epoch 72/100\n",
            "11/11 - 0s - loss: 0.0057\n",
            "Epoch 73/100\n",
            "11/11 - 0s - loss: 0.0056\n",
            "Epoch 74/100\n",
            "11/11 - 0s - loss: 0.0056\n",
            "Epoch 75/100\n",
            "11/11 - 0s - loss: 0.0054\n",
            "Epoch 76/100\n",
            "11/11 - 0s - loss: 0.0052\n",
            "Epoch 77/100\n",
            "11/11 - 0s - loss: 0.0052\n",
            "Epoch 78/100\n",
            "11/11 - 0s - loss: 0.0050\n",
            "Epoch 79/100\n",
            "11/11 - 0s - loss: 0.0050\n",
            "Epoch 80/100\n",
            "11/11 - 0s - loss: 0.0049\n",
            "Epoch 81/100\n",
            "11/11 - 0s - loss: 0.0048\n",
            "Epoch 82/100\n",
            "11/11 - 0s - loss: 0.0049\n",
            "Epoch 83/100\n",
            "11/11 - 0s - loss: 0.0047\n",
            "Epoch 84/100\n",
            "11/11 - 0s - loss: 0.0046\n",
            "Epoch 85/100\n",
            "11/11 - 0s - loss: 0.0046\n",
            "Epoch 86/100\n",
            "11/11 - 0s - loss: 0.0043\n",
            "Epoch 87/100\n",
            "11/11 - 0s - loss: 0.0043\n",
            "Epoch 88/100\n",
            "11/11 - 0s - loss: 0.0043\n",
            "Epoch 89/100\n",
            "11/11 - 0s - loss: 0.0041\n",
            "Epoch 90/100\n",
            "11/11 - 0s - loss: 0.0040\n",
            "Epoch 91/100\n",
            "11/11 - 0s - loss: 0.0040\n",
            "Epoch 92/100\n",
            "11/11 - 0s - loss: 0.0040\n",
            "Epoch 93/100\n",
            "11/11 - 0s - loss: 0.0038\n",
            "Epoch 94/100\n",
            "11/11 - 0s - loss: 0.0038\n",
            "Epoch 95/100\n",
            "11/11 - 0s - loss: 0.0037\n",
            "Epoch 96/100\n",
            "11/11 - 0s - loss: 0.0036\n",
            "Epoch 97/100\n",
            "11/11 - 0s - loss: 0.0036\n",
            "Epoch 98/100\n",
            "11/11 - 0s - loss: 0.0036\n",
            "Epoch 99/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Epoch 100/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 4)                 68        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 73\n",
            "Trainable params: 73\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd2jZl4n0H8m"
      },
      "source": [
        "### 1.3 Evaluate Predictive Model Performance (10 Points)\n",
        "\n",
        "Predict datapoints with the observed datapoints and trained model. \n",
        "\n",
        "**Tasks:**\n",
        "1. Do direct prediction on train and test datapoints with the obtained model in section 1.2. **(2 Points)**\n",
        "2. Scale the prediction results back to original representation with the scaler. **(3 Points)**\n",
        "3. Calculate root mean squared error (RMSE) and **print out** the error for **both TRAIN and TEST**. **(3 Points)**\n",
        "4. **Plot** the **TEST** label and prediction. **(2 Points)**\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Scale back the predictions with the build-in function \"scaler.inverse_transform\".\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform\n",
        "2. For validation: Train Score: **~13 RMSE** Test Score: **~19 RMSE**\n",
        "3. The plot for validation is shown below (observation test data are blue and prediction results are orange):\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEkAMxnz8Mq"
      },
      "source": [
        "### Make Predictions ###\n",
        "\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbnRqEv9z-he"
      },
      "source": [
        "### Scale Back Predictions ###\n",
        "\n",
        "trainPredict = scaler.inverse_transform(trainPredict) # scale train prediction back with scaler.inverse_transform()\n",
        "trainY = scaler.inverse_transform(trainY)  # scale train labels back with scaler.inverse_transform()\n",
        "\n",
        "testPredict = scaler.inverse_transform(testPredict) # scale test prediction back with scaler.inverse_transform()\n",
        "testY = scaler.inverse_transform(testY) # scale test labels back with scaler.inverse_transform()\n"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdBWzmE91G6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a6f0d3-abc7-49b1-8f69-d479aa53ed31"
      },
      "source": [
        "### Calculate Root Mean Squared Error (RMSE) ###\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error # Import mean_squared_error from sklearn.metrics\n",
        "\n",
        "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0])) \n",
        "testScore = math.sqrt(mean_squared_error(testY, testPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 21.22 RMSE\n",
            "Test Score: 66.95 RMSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txdu8q7l1aju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "3c519a77-e712-4e25-b41a-c632060ad743"
      },
      "source": [
        "### Plot Baseline and Predictions ###\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(testY) \n",
        "plt.plot(testPredict)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7ae02a05f8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49Ug-FhtCg8"
      },
      "source": [
        "## 2 - Build an LSTM model to conduct sentiment analysis ##\n",
        "\n",
        "### 2.1 Prepare the data (13 Points) ###\n",
        "\n",
        "Prepare IMDB data for reccurent neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the data from IMDB review dataset and **print out** the lengths of sequences. **(3 Points)**\n",
        "2. Preprocess review data to meet the network input requirement by specifying **number of words=1000**, setting **the analysis length of the review = 100**, and **padding the input sequences**. **(10 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may load the IMDB data with keras.datasets.imdb.load_data(num_words=max_features). Here. max_features is set to **1000**.\n",
        "2. You may use keras.preprocessing.sequence.pad_sequences(x_train, maxlen) to pad the input sequences and set maxlen to **100**.\n",
        "\n",
        "**Note:**\\\n",
        "We train the built LSTM-based model with ALL training data; the **validation set** (aka **development set**) is set with the **testing set** for model evaluation. This split is common in the application with limited sampled observation data, like NLP problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI4ki461S2V3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvV1Sv2a18SM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9593f5f-93c8-450c-b226-271b747a0aa0"
      },
      "source": [
        "# Prepare the data here\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words = max_features ) # load IMDB data with specified num_words = 1000; testing set is set to validation set.\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen) # Pad IMDB training data with specified maxlen=100\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val,maxlen=maxlen) # Pad IMDB validation data with specified maxlen=100\n",
        "\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JFQeWK18SR"
      },
      "source": [
        "### 2.2 - Design and train LSTM model (25 Points) ###\n",
        "\n",
        "Build an LSTM model.\n",
        "\n",
        "**Tasks:**\n",
        "1. Build the LSTM model with **1 embedding layer**, **1 LSTM layer**, and **1 Dense layer**. **Print out** model summary. The embedding vector is specified with the dimension of **8**. **(10 Points)**\n",
        "2. Compile the LSTM model with **Adam** optimizer, **binary_crossentropy** loss function, and **accuracy** metrics. **(5 Points)**  \n",
        "3. Train the LSTM model with **batch_size=64 for 10 epochs** and report **training and validation accuracies over epochs**. **(5 Points)**\n",
        "4. **Print out** best validation accuracy. **(5 Points)**\n",
        "\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Set input dimension to **1000** and output dimension to **8** for embedding layer.\n",
        "2. Set **unit_size=8** for LSTM layer.\n",
        "3. Set activation function to **sigmoid** for Dense layer.\n",
        "4. For validation: the outputs for first epoch should be close to（but maybe not exactly following） the statistics below:\\\n",
        "**loss: ~0.5675 - accuracy: ~0.7072 - val_loss: ~0.4549 - val_accuracy: ~0.8020**\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDqqgFt118SS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6b4b45-c200-4e1a-c013-7e20a87d9e8c"
      },
      "source": [
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features,8)(inputs) # Embed data in an 8-dimensional vector\n",
        "x = layers.LSTM(8)(x) # Add 1st layer of LSTM with 8 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1,activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam',metrics=[\"accuracy\"],loss='binary_crossentropy') # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train,y_train,epochs=10,batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model with model.fit()"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 8)           8000      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 8)                 544       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 8,553\n",
            "Trainable params: 8,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.5672 - accuracy: 0.7074 - val_loss: 0.4455 - val_accuracy: 0.8043\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 17s 43ms/step - loss: 0.4145 - accuracy: 0.8163 - val_loss: 0.4000 - val_accuracy: 0.8192\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 18s 47ms/step - loss: 0.3836 - accuracy: 0.8330 - val_loss: 0.3845 - val_accuracy: 0.8266\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3681 - accuracy: 0.8399 - val_loss: 0.3832 - val_accuracy: 0.8270\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3623 - accuracy: 0.8426 - val_loss: 0.3814 - val_accuracy: 0.8286\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 18s 45ms/step - loss: 0.3595 - accuracy: 0.8417 - val_loss: 0.4315 - val_accuracy: 0.8161\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 18s 46ms/step - loss: 0.3533 - accuracy: 0.8432 - val_loss: 0.3719 - val_accuracy: 0.8310\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 18s 45ms/step - loss: 0.3467 - accuracy: 0.8473 - val_loss: 0.3730 - val_accuracy: 0.8310\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 17s 44ms/step - loss: 0.3406 - accuracy: 0.8486 - val_loss: 0.3891 - val_accuracy: 0.8313\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 17s 44ms/step - loss: 0.3357 - accuracy: 0.8518 - val_loss: 0.3821 - val_accuracy: 0.8302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7ade02d518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqvy2tdEw7J"
      },
      "source": [
        "### 2.3 - LSTM hyperparameter tuning (Bonus 15 Points) ###\n",
        "\n",
        "Boost the performance of obtained LSTM (aka vanilla model) by hyperparameter tuning.\n",
        "\n",
        "**Tasks:**\n",
        "- All modificiations are directly conducted based on the vanilla model above (from 2.2).\n",
        "- For each scenario, **report <span style=\"color:red\"> BEST Validation Accuracy </span> and generate Training/Validation <span style=\"color:red\"> Accuracy plots over epochs</span>**. You may just paste the plot figures in the cells with **Markdown mode**. Make sure it is correctly shown after jupyter notebook run the cell.\n",
        "1.  Scenario 1 (**5 points**):\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 16.\n",
        "    - Modify the units of LSTM to 16.\n",
        "2. Scenario 2 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "3. Scenario 3 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "    - Increase analysis length for review data to maxlen = 200\n",
        "\n",
        "**Hints:**  \n",
        "For validation: the outputs for first epoch should be close to（but maybe not exactly following） the statistics below:\n",
        "- Scenario 1: **loss: ~0.4968 - accuracy: ~0.7450 - val_loss: ~0.4079 - val_accuracy: ~0.8198**\n",
        "- Scenario 2: **loss: ~0.4764 - accuracy: ~0.7670 - val_loss: ~0.4133 - val_accuracy: ~0.8179**\n",
        "- Scenario 3: **loss: ~0.4819 - accuracy: ~0.7644 - val_loss: ~0.4031 - val_accuracy: ~0.8105**\n",
        "\n",
        "You may follow the example from the reference below to add additional LSTM layer.\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keod5xXkEKnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553af51b-b358-42b6-c693-636bf99feeed"
      },
      "source": [
        "########################### Scenario 1 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features,16)(inputs) # Embed data in a 16-dimensional vector\n",
        "x = layers.LSTM(16,return_sequences='true')(x) # Add 1st layer of LSTM with 16 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(16)(x) # Add 2nd layer of LSTM with 16 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam',metrics=[\"accuracy\"],loss='binary_crossentropy' ) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train,y_train,epochs=10,batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 16)          16000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 16)          2112      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 16)                2112      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 20,241\n",
            "Trainable params: 20,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.4968 - accuracy: 0.7450 - val_loss: 0.4079 - val_accuracy: 0.8198\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.3883 - accuracy: 0.8286 - val_loss: 0.3844 - val_accuracy: 0.8242\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 37s 95ms/step - loss: 0.3688 - accuracy: 0.8382 - val_loss: 0.3692 - val_accuracy: 0.8333\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 36s 93ms/step - loss: 0.3541 - accuracy: 0.8436 - val_loss: 0.3798 - val_accuracy: 0.8257\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 37s 94ms/step - loss: 0.3446 - accuracy: 0.8464 - val_loss: 0.3665 - val_accuracy: 0.8338\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 36s 92ms/step - loss: 0.3353 - accuracy: 0.8520 - val_loss: 0.4104 - val_accuracy: 0.8247\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 36s 93ms/step - loss: 0.3257 - accuracy: 0.8540 - val_loss: 0.3680 - val_accuracy: 0.8325\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 39s 100ms/step - loss: 0.3188 - accuracy: 0.8594 - val_loss: 0.3659 - val_accuracy: 0.8334\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.3110 - accuracy: 0.8602 - val_loss: 0.3800 - val_accuracy: 0.8334\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 38s 97ms/step - loss: 0.3048 - accuracy: 0.8656 - val_loss: 0.3928 - val_accuracy: 0.8343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7adf38a518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnloPg7523rU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0301ad86-374e-4ce2-ac42-7bd10a93f910"
      },
      "source": [
        "########################### Scenario 2 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features,128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128,return_sequences='true')(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1,activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam',metrics=[\"accuracy\"],loss='binary_crossentropy') # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train,y_train,epochs=10,batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 128)         128000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 128)         131584    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 241s 618ms/step - loss: 0.4743 - accuracy: 0.7661 - val_loss: 0.3955 - val_accuracy: 0.8220\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 234s 598ms/step - loss: 0.3813 - accuracy: 0.8316 - val_loss: 0.3756 - val_accuracy: 0.8325\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 236s 603ms/step - loss: 0.3550 - accuracy: 0.8413 - val_loss: 0.3641 - val_accuracy: 0.8360\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 240s 613ms/step - loss: 0.3312 - accuracy: 0.8560 - val_loss: 0.3683 - val_accuracy: 0.8398\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 235s 601ms/step - loss: 0.3113 - accuracy: 0.8665 - val_loss: 0.3551 - val_accuracy: 0.8456\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 240s 613ms/step - loss: 0.3043 - accuracy: 0.8695 - val_loss: 0.3761 - val_accuracy: 0.8404\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 235s 602ms/step - loss: 0.2840 - accuracy: 0.8796 - val_loss: 0.3872 - val_accuracy: 0.8234\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 235s 602ms/step - loss: 0.2768 - accuracy: 0.8830 - val_loss: 0.3720 - val_accuracy: 0.8424\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 235s 602ms/step - loss: 0.2473 - accuracy: 0.8981 - val_loss: 0.3896 - val_accuracy: 0.8343\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 235s 601ms/step - loss: 0.2314 - accuracy: 0.9041 - val_loss: 0.3910 - val_accuracy: 0.8411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7adacc1ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwtHVaGM23rV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "outputId": "596c1245-8397-49dc-8746-3ea75561d458"
      },
      "source": [
        "########################### Scenario 3 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000# Only consider the top 1k words\n",
        "maxlen = 200 # Only consider the first 200 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(max_features,128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(128,return_sequences='true')(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1,activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(optimizer='adam',metrics=[\"accuracy\"],loss='binary_crossentropy') # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train,y_train,epochs=10,batch_size=64, validation_data=(x_val, y_val)) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 128)         128000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 128)         131584    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 233s 597ms/step - loss: 0.4743 - accuracy: 0.7661 - val_loss: 0.3955 - val_accuracy: 0.8220\n",
            "Epoch 2/10\n",
            "159/391 [===========>..................] - ETA: 1:45 - loss: 0.3706 - accuracy: 0.8397"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-183-da4a46208b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}