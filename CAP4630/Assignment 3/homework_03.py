# -*- coding: utf-8 -*-
"""Homework_03 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XEc8eukPpqD4IVaaYclXGth9tFQSKAHg

# Convolution Neural Network Homework

This is the 3rd assignment for CAP 4630 and we will go through some primary operations for image processsing and implement one of the earilest but representative convolution neural networks - LeNet-5 . \
You will use **"Tasks"** and **"Hints"** to finish the work. **(Total 100 Points)** \
You are **not** allowed to use Machine Learning libaries such as Scikit-learn and Keras for first section; but you are encouraged to employ Keras for second section.

**Task Overview:**
- Basic operations for Digital Image Processing (DIP)
- LeNet-5

## 1 - Basic Image Processing ##
### 1.1 Data Preparation 

Import useful packages and prepare image data as an array for image processing. **(5 Points)**

**Tasks:**
1. Import numpy and rename it to np.
2. Import imageio and call imread to convert image to an array.
3. Show the image in the output box before image-array conversion.
4. Print out the size of the array
5. Print out the numeric matrix form of image, i.e. the obtained array after image-array conversion.

References:
- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.
- [imageio](https://imageio.github.io/) is a python library for basic image reading and writing.

**Hints:**
1. Image data is under current directory, i.e., "./image.jpg".
2. You may consider importing "display" and "Image" from IPython.display for image display.
"""

# Import useful libraries
import numpy as np
import imageio
from IPython.display import Image
from IPython.display import display
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Display original image
img = Image.open('image.jpg')
display(img)

# Convert image to array, print out the shape of array, and print out the entire array
im = imageio.imread('image.jpg')
print(im.shape)
print(im)



"""### 1.2 Implementation of Convolution Filter

Process the obtained array from the image with convolution operation. **(20 Points)**

**Tasks:**
1. Prepare a 3X3 Laplacian kernel (aka Laplacial filter) with array as convolution filter.
2. Conduct convolution on image with prepared kernel.
3. Print out convolution result for first ten rows.
4. Print out the shape of the convolution result.
5. Display convolution result as image with matplotlib. (Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)


**Hints:**
1. Laplacian kernel is widely used for edge detection. Its form is shown below:
![image.png](attachment:image.png)
2. You may consider the following steps for this implementation.\
    2.1 Extract all centriods of processing region for each convolution operation.\
    2.2 According to each centroid, locate all indices of the elements within the local region for each convolution operation.\
    2.3 Given obtained indices, locate pixel values (i.e. our obtained array elements) and conduct element-wise product between pixel and kernel values.\
    2.4 Sum element-wise product results and assign the value to convolution result at corresponding location.\
    **Note: we did not conduct padding for processed array, and thus, convolution result will become smaller than original array. You may think about the reason.**
3. Validation for first 5X5 array (from upper-left corner), i.e., filtered_results[0:5,0:5]. The example figure is below.

[[ 134.   37.   98.  195.  173.]\
 [ -75.  -80.   56.  -65.  182.]\
 [  96.  -37. -163.   22.   68.]\
 [-101.  121.   81.  148.  -71.]\
 [   7.  127. -141.  159. -127.]]

![image.png](attachment:image.png)
"""

######## Convolution with Laplacian Filter ##################
conv_filt = np.array([[0, 1, 0], 
                      [1,-4, 1],
                      [0, 1, 0]])


#(30 - 3 + 0P)/1 + 1
rows, cols = (28, 28) 
conv_image = np.array([[0]*cols]*rows) 
#[i   j, i   j+1, i   j+2], 
#[i+1 j, i+1 j+1, i+1 j+2],
#[i+2 j ,i+2 j+1, i+2 j+2]

def kek(i,j):
  lol = np.array([[im[i][j], im[i][j+1], im[i][j+2]], 
                  [im[i+1][j], im[i+1][j+1], im[i+1][j+2]],
                  [im[i+2][j], im[i+2][j+1], im[i+2][j+2]]])
  return lol


for i in range(28):
  for j in range (28):
      conv_image[i][j] = sum(sum(conv_filt * kek(i,j)))
      #print ("i:",i,"j",j)
      #print(sum(sum(conv_filt * kek(i,j))))
      #print("------->",conv_image[i][j])
      #print("xxxxxxx>",conv_image[0][0])
      #print ("=========================")

print(conv_image[0:5,0:5])
imgplot = plt.imshow(conv_image)

"""### 1.3 Modification on Convolution Scheme

Conduct the convolution with the same filter as above, but change the stride to 2. **(5 Points)**

**Tasks:**
1. Modify the convolution process with stride=2
2. Print out convolution result for first ten rows.
3. Print out the shape of the convolution result.
4. Display convolution result as image with matplotlib.(Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)

**Hints:**
1. You may just reduce the centroid pool according to stride=2, and then, follow the same convolution process above.
    **Note: After increase of stride, the size of convolution result is further shrinked. You may think about the reason.**
2. Validation for first 5X5 array (from upper-left corner), i.e., filtered_results[0:5,0:5]. The example figure is below.

[[ 134.   98.  173.    5.    3.]\
 [  96. -163.   68.  -10.   37.]\
 [   7. -141. -127.  142.   -6.]\
 [  -1.  -46.  109.  -13.   11.]\
 [ 106.   49.  241.  -26.  -33.]]
 
![image.png](attachment:image.png) 
"""

######## Convolution with Laplacian Filter and the setting of stride=2 ##################
i = 0
j = 0
row = 0
cal = 0

#floor[((30 - 3 + 0P)/2 + 1] 
rows, cols = (14, 14) 
conv_image2 = np.array([[0]*cols]*rows) 
#[i   j, i   j+1, i   j+2], 
#[i+1 j, i+1 j+1, i+1 j+2],
#[i+2 j ,i+2 j+1, i+2 j+2]

def kek1(row,cal):
  lol1 = np.array([[im[row][cal], im[row][cal+1], im[row][cal+2]], 
                  [im[row+1][cal], im[row+1][cal+1], im[row+1][cal+2]],
                  [im[row+2][cal], im[row+2][cal+1], im[row+2][cal+2]]])
  return lol1


for i in range(14):
  for j in range (14):
      #print ("row:",row,"cal:",cal)
      conv_image2[i][j] = sum(sum(conv_filt * kek(row,cal)))
      cal = cal + 2
      #print ("i:",i,"j: ",j)
      #print(sum(sum(conv_filt * kek(i,j))))
      #print("------->",conv_image2[i][j])
      #print("xxxxxxx>",conv_image2[0][0])
      #print ("=========================")
  row = row + 2
  cal = 0

print(conv_image2[0:5,0:5])
imgplot = plt.imshow(conv_image2)

"""### 1.4 Implementation of MaxPooling

Process the obtained array from the image with MaxPooling operation. **(10 Points)**

**Tasks:**
1. Prepare a 2X2 pooling mask.
2. Conduct max pooing on image with prepared mask.
3. Print out convolution result for first ten rows.
4. Print out the shape of the convolution result.
5. Display convolution result as image with matplotlib.(Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)

**Hints:**
1. You may just modify the centroid pool to top-left corner pool, and then, follow the same strategy above.\
    **Note: After the pooling, the size of the array is shrinked. You may think about the reason.**
2. Validation for first 5X5 array (from upper-left corner), i.e., pooled_results[0:5,0:5].The example figure is below.

[[ 98. 112.  93. 195. 173.]\
 [ 84. 127. 137. 253. 254.]\
 [ 85. 145. 225. 255. 242.]\
 [104. 178. 216. 230. 242.]\
 [ 95. 186. 147. 248. 242.]]
 ![image.png](attachment:image.png)
"""

######## MaxPooling with the setting of 2X2 ##################
i = 0
j = 0
row = 0
cal = 0

# Hand counted 30/2
rows, cols = (15, 15) 
pooling_res = np.array([[0]*cols]*rows) 
#[i   j, i   j+1, i   j+2], 
#[i+1 j, i+1 j+1, i+1 j+2],
#[i+2 j ,i+2 j+1, i+2 j+2]

def kek2(row,cal):
  lol1 = np.array([[im[row][cal], im[row][cal+1]], 
                  [im[row+1][cal], im[row+1][cal+1]]])
  return lol1


for i in range(15):
  for j in range (15):
      #print ("row:",row,"cal:",cal)
      x = kek2(row, cal)
      #print(x.max())
      pooling_res[i][j] = x.max()
      cal = cal + 2
      #print ("i:",i,"j: ",j)
      #print(sum(sum(conv_filt * kek(i,j))))
      #print("------->",conv_image2[i][j])
      #print("xxxxxxx>",conv_image2[0][0])
      #print ("=========================")
  row = row + 2
  cal = 0

print(pooling_res[0:5,0:5])
imgplot = plt.imshow(pooling_res)

"""## 2 - Convolution Neural Network ##
 
In this section, we will use LeNet-5 (LeCun et al., 1998), one of representative deep nueral networks, to solve a  classification problem with Fashion MNIST benchmark.

### 2.1 Library Preparation

Import useful deep learning packages. 

**Tasks:**
1. Import numpy and rename it to np.
2. Import keras from tensorflow.
3. Import layers from tensorflow.keras

"""

import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

"""### 2.2 Training Data Preparation

Import useful packages and prepare Fashion MNIST data. **(15 Points)**

**Tasks:**
1. Download Fashion MNIST data and split it with keras and prepare training/test data sets.
2. Preprocess training/test data with normalization, dimension extension, and padding (for LeNet-5 configuration).
3. Preprocess label data to binary class matrices.
4. Print out first image in training set and its correponding label index
5. Print out the shape of total training data, the number of training samples, and the number of test samples.

**Hints**
1. You may consider load function from the reference link. https://keras.io/api/datasets/ It provides dataloader function which can tackle downloading and data splitting automatically.
2. For label preprocessing, you may consider **keras.utils.to_categorical** to convert class vectors to binary class matrices. This conversion makes sure the label can match the format of prediction output from neural network.
3. For image display, consider showing the image and label **before dimension extension and label preprocessing**.
4. You may consider MNIST processing shown in class as an example.

**References**
- Fashion MNIST https://github.com/zalandoresearch/fashion-mnist

"""

# the data, split between train and test sets
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# Image Normalization (Scaling to [0, 1])
train_images = train_images.astype("float32") / 255
test_images = test_images.astype("float32") / 255

# Print out first image and its correponding label index
print(train_images[0])
print(train_labels[0])

# Dimension extension to ensure that images have shape (28, 28, 1)


# Conduct padding on training/test images to (32, 32, 1) for LeNet-5

# Print out the training/test data shapes and the numbers of training/test samples

# convert label vectors to binary class matrices for training/test labels

"""### 2.2 LeNet-5 

Construct LeNet-5 as learning model for Fashion MNIST classification task. **(15 Points)**

**Tasks:**
1. Build up LeNet-5 with keras.Sequential
2. Print out the **model summary**.

**Hints:**
1. You may consider the convolution neural network shown in class as an example.
2. The structure of LeNet-5 is listed below. Try to map each step to related processing operation. You can also search some materials to faciliate implementation. 
3. Kindly note that we modify some original settings here. \
    First, we change tanh activation to "relu" activation here. Please use "**activation="relu"**" for implementation.\
    Next, we use MaxPooling instead of original AveragePooling. Please use "**MaxPooling2D(pool_size=(2, 2))**" for implementation.
![image.png](attachment:image.png)

**References:**
- http://yann.lecun.com/exdb/lenet/

"""

### Construct LeNet-5 and Print out Model Summary

"""### 2.3 LeNet-5 Model Training

Train LeNet-5 model with specific training strategy. **(15 Points)**

**Tasks:**
1. Set batch size to **64** for training. 
2. Pick **SGD optimizer** with learning rate of **0.1**, learning rate decay of **4e-5** (i.e., decay=4e-5), momentum of **0.9**, and **nesterov=True**, for model training.
3. Pick **cross-entropy** loss function for optimization and evaluation metrics is set to **accuracy**.
4. Set validation_split to **0.1** which means it excludes 1/10 training data for validation process.
4. Train the model with **10 epochs**.
5. Evaluate model with test data set and print out **test loss** and **test accuracy**.

**Hints:**
1. You may consider the examples from Keras to specify optimizer parameters. https://keras.io/api/optimizers/
2. You may use the example shown in class to faciliate this implementation.
3. You may see slightly different results every time you run the training. It is normal since there is randomness for training. However, you should expect the validation accuracy is above **87%**.
"""

### Train with SGD optimizer with learning rate =0.1, regularizer=4e-5, momentum=0.9, and nesterov=True

### Print out the evaluation results, including test loss and test accuracy.

"""### 2.4 Different Training Schemes

Train LeNet-5 model with different strategies. **(15 Points)**\
You may put reported numbers and plots in one figure or screenshot. \
Please insert the resulting graphs or screenshots in below box in **Markdown** Mode.

**Tasks:**
1. Try to set batch size to **128** and see if there is a difference. \
    Report **final test loss and test accuracy** and **intermediate training/validation loss plots over epochs**. 
2. Try to set optimizer to **Adam** (optimizer="adam") and see if there is a difference. \
    Report **final test loss and test accuracy** and **intermediate training/validation loss plots over epochs**. 
3. Try to change learning rate to **0.05** and see if there is a difference. \
    Report **final test loss and test accuracy** and **intermediate training/validation loss plots over epochs**. 

**Hints:**
1. You may refresh the model every time to make sure it starts the training from scratch.
2. Each modification only changes ONE hyperparameter. For example, for Adam optimizer, please keep batch size of 64; for learning rate change, please keep batch size of 64 and SGD optimizer.

**First graph**

**Second graph**

**Third graph**
"""