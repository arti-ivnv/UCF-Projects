# -*- coding: utf-8 -*-
"""Homework_01 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X0HKVmnuCDj5UER5Zfc51Kjmj9NHGDC0

# Regression Homework

This is the first assignment for CAP 4630 and we will develop two basic models with regression. \
You will use **"Tasks"** and **"Hints"** to finish the work. **(Total 100 Points)** \
You are **not** allowed to use Machine Learning libaries such as Scikit-learn and Keras.

**Task Overview:**
- Singal Variable Nonlinear Regression 
- Multiple Variable Linear Regression

## 1 - Packages ##

Import useful packages for scientific computing and data processing. **(5 Points)**

**Tasks:**
1. Import numpy and rename it to np.
2. Import pandas and rename it to pd.
3. Import the pyplot function in the libraray of matplotlib and rename it to plt.

References:
- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.
- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.

**Attention:**
1. After this renaming, you will use the new name to call functions. For example, **numpy** will become **np** in the following sections.
"""

# Import and rename libraries here

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""## 2 - Data Preparation ##

Prepare the data for regression task. **(10 Points)**

**Tasks:**
1. Load data for nonlinear regression.
2. Generate the scatter plot of the data.

**Hints:**
1. The data file is "data_nonlinear.csv".
2. The data format is as follows: 1st column is X and 2nd column is Y.
3. You may follow the example in class.
"""

# Preprocess input data and generate plots
plt.rcParams['figure.figsize'] = (8.0, 6.0)

data = pd.read_csv('data_nonlinear.csv')
X = data.iloc[:, 0]
Y = data.iloc[:, 1]
plt.scatter(X, Y)
plt.show()

"""## 3 - Single Variable Nonlinear Regression ##


Develop a regression model, estimate coefficients with data, and derive the relationship. **(30 Points)**

**Tasks:**
1. Establish a relationship between Y and X with a quadratic function.
2. Compute MSE loss with observation-prediction pairs. 
3. Implement Stochastic Gradient Descent (SGD) to achieve optimal solution with the learning rate of **0.0001** and 10000 epochs.
4. Print out the optimal solution at final step. 

**Hints:**  
1. Given the example of linear regression in class, modify the function to an equation for a parabola with coefficients of **a** , **b**, and **c** for qudractic, linear, and constant term.
2. Initialize the model with zero. For example, a=0, b=0, and c=0.
3. It may take **10-15 seconds**  to finish the running for 10000 steps. Be patient.
3. For debugging, the results of **a**, **b**, and **c** for first five steps are as follows:

Epoch 0: 3.522850763234379 &nbsp; 0.4636117434077167 &nbsp; 0.0657326585304121 \
Epoch 1: 5.863455084318797 &nbsp; 0.7717473808944446 &nbsp; 0.1094845844539659 \
Epoch 2: 7.418562189587087 &nbsp; 0.9765829680329118 &nbsp; 0.1386322511155162 \
Epoch 3: 8.451776522837656 &nbsp; 1.1127848361152428 &nbsp; 0.1580766665797253 \
Epoch 4: 9.138239705471658 &nbsp; 1.2033856677109787 &nbsp; 0.1710741132129369

"""

a = 0
b = 0
c = 0

L = 0.0001  # The learning Rate
epochs = 10000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X

# print(float(len(X)))

# print(float(len(Y)))

# Performing Gradient Descent 
for i in range(epochs):
    Y_pred = a * (X ** 2) + b * X + c  # The current predicted value of Y
    D_a = (-2/n) * sum((X ** 2) * (Y - Y_pred))  # Derivative wrt a
    D_b = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt b
    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c
    a = a - L * D_a  # Update a
    b = b - L * D_b  # Update b
    c = c - L * D_c  # Update c
    # print("echo" , i, ": ", a, b, c) # Debugging output
# print("===================")
print (a,b, c)

"""## 4 - Prediction Results ##

Derive prediction function and generate estmated results. **(5 Points)**

**Tasks:**
1. Derive prediction function with the obtained coefficients above.
2. Generate scatter plots for original data pairs X-Y and prediction results X-Y_Pred in the same figure.

**Hint:**
1. You may follow the example in class materials.
2. An example is shown below.

![mxplusc](Single_Variable_Nonlinear_Regression.png)
"""

# Generate prediction graphs here
plt.scatter(X, Y)
plt.scatter(X, Y_pred, color = 'red') # predicted
plt.show()

"""## 5 - Multiple Variables Linear Regression ##

## 5.1 Data Preparation

Prepare the data for regression task. **(10 Points)**

**Tasks:**
1. Load data for multiple variable linear regression.
2. Generate the 3D scatter plot of the data.

**Hints:**
1. The data file is "data_two_variables.csv".
2. The data format is as follows: 1st column is X1, 2nd column is X2, and 3rd colum is Y.
3. You may use "mplot3d" in the toolkit of "mpl_toolkits" and import "Axes3D" to faciliate 3D scatter plot. More details can be found in the reference of https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html

"""

from mpl_toolkits.mplot3d import Axes3D

data = pd.read_csv('data_two_variables.csv')
X1 = data.iloc[:, 0]
X2 = data.iloc[:, 1]
Y = data.iloc[:, 2]

fig = plt.figure(figsize=(8.0,6.0))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(X1, X2, Y)

"""
## 5.2 Linear Regression

Develop a regression model, estimate coefficients with data, and derive the relationship. **(30 Points)**

**Tasks:**
1. Establish a linear function to describe the relationship among Y, X1, and X2.
2. Compute MSE loss with observation-prediction pairs 
3. Implement Stochastic Gradient Descent (SGD) to achieve optimal solution with the learning rate of **0.001**  and 10000 epochs.
4. Print out the optimal solution at final step. 


**Hints:**  
1. Given the example of linear regression in class, modify the function to a linear equation with two independent variables X1 and X2. The coefficients of X1 and X2 are **m1** and **m2**, respectively. The constant term is **m3**.
2. Initialize the model with zero. For example, m1=0, m2=0, and m3=0.
3. It may take **10-15 seconds**  to finish the running for 10000 steps. Be patient.
4. For debugging, the results of **m1**, **m2**, and **m3** for first five steps are as follows:

Epoch 0: 0.626270311781417 &nbsp; 1.2878873109656148 &nbsp; 0.131853565020141 \
Epoch 1: 1.094591701874305 &nbsp; 2.2053564293772494 &nbsp; 0.232875943398126 \
Epoch 2: 1.448920621209634 &nbsp; 2.8567953507664927 &nbsp; 0.311709041900032 \
Epoch 3: 1.720861720010732 &nbsp; 3.3172257148527318 &nbsp; 0.374564770080451 \
Epoch 4: 1.933141750092154 &nbsp; 3.6405628341256704 &nbsp; 0.425908509604264"""

# Build the model
m1 = 0
m2 = 0
m3 = 0

L = 0.001  # The learning Rate
epochs = 10000  # The number of iterations to perform gradient descent

n = float(len(X)) # Number of elements in X

# print(float(len(X)))

# print(float(len(Y)))

# Performing Gradient Descent 
for i in range(epochs):
    Y_pred = m1 * X1 + m2 * X2 + m3  # The current predicted value of Y
    D_m1 = (-2/n) * sum(X1 * (Y - Y_pred))  # Derivative wrt m1
    D_m2 = (-2/n) * sum(X2 * (Y - Y_pred))  # Derivative wrt m2
    D_m3 = (-2/n) * sum(Y - Y_pred)  # Derivative wrt m3
    m1 = m1 - L * D_m1  # Update m1
    m2 = m2 - L * D_m2  # Update m2
    m3 = m3 - L * D_m3  # Update m3
    #print("echo" , i, ": ", m1, m2, m3) # Debugging output
# print("===================")
print (m1,m2, m3)

"""
## 5.3 - Prediction Results ##

Derive prediction function and generate estmated results. **(10 Points)**

**Tasks:**
1. Derive prediction function with the obtained coefficients above.
2. Generate 3D scatter plots for original data pairs X-Y and prediction results X-Y_Pred in the same figure.

**Hint:**
1. You may follow the example above.
2. An example is shown below.

![mxplusc](Multiple_Variable_Linear_Regression.png)"""

# Generate prediction graphs here
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(X1, X2, Y)
ax.scatter(X1, X2, Y_pred, c = 'red')

